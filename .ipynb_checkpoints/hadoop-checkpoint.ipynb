{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Hadoop is an open source version inspired by Google MapReduce and Google File System and designed for distributed processing of large data sets across a cluster of systems. Hadoop is designed with an assumption that all hardware fails sooner or later and that the system should be robust and able to handle the hardware failures automatically. This is why hadoop is considered to be <b>fault tolerant</b>, since the goal is to have nodes fail without suffering any loss of data.\n",
    "\n",
    "Apache Hadoop consists of two core components:\n",
    "\n",
    "\n",
    "### HDFS\n",
    "\n",
    "The Hadoop Distributed File System is a filesystem that manages the storage across a network of machines. This is an incredibly complex problem to handle without the use of big data technology like hadoop. \n",
    "\n",
    "### MapReduce\n",
    "\n",
    "Framework and API for MapReduce jobs. MapReduce is a software framework or programming model, which enable users to write programs so that data can be processed parallelly across multiple systems in a cluster. MapReduce consists of two parts: \n",
    "\n",
    "\n",
    "####  Map \n",
    "\n",
    "Map task is performed using a `map()` function that basically performs filtering and sorting. This part is responsible for processing one or more chunks of data and producing the output results which are generally referred as intermediate results. As shown in the diagram below, map task is generally processed in parallel provided the maping operation is independent of each other.\n",
    "\n",
    "\n",
    "#### Reduce\n",
    "\n",
    "Reduce task is performed by `reduce()` function and performs a summary operation. It is responsible for consolidating the results produced by each of the Map task.\n",
    "\n",
    "\n",
    "### Processing Patterns\n",
    "\n",
    "There are different ways in which hadoop processes its data, four of which are overviewed below:\n",
    "\n",
    "1. Batch processing: Batch Processing is Hadoop's classic feature and is specifically when the processing job is known and ready to be run over large amounts of data.\n",
    "\n",
    "2. Interactive SQL: \n",
    "\n",
    "3. Iterative processing: Many algorithms - such as those in machine learning - are iterative in nature, so it’s much more efficient to hold each intermediate working set in memory, compared to loading from disk on each iteration. The architecture of MapReduce does not allow this, but it’s straightforward with Spark, for example, and it enables a highly exploratory style of working with datasets.\n",
    "\n",
    "4. Stream processing: Streaming systems like Storm, Spark Streaming, or Samza make it possible to run real-time, distributed computations on unbounded streams of data and emit results to Hadoop storage or external systems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def read_input(file):\n",
    "    for line in file:\n",
    "        # split the line into words\n",
    "        yield line.split()\n",
    "\n",
    "def main(separator='\\t'):\n",
    "    # input comes from STDIN (standard input)\n",
    "    data = read_input(sys.stdin)\n",
    "    for words in data:\n",
    "        # write the results to STDOUT (standard output);\n",
    "        # what we output here will be the input for the\n",
    "        # Reduce step, i.e. the input for reducer.py\n",
    "        #\n",
    "        # tab-delimited; the trivial word count is 1\n",
    "        for word in words:\n",
    "            print('%s%s%d' % (word, separator, 1))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"A more advanced Reducer, using Python iterators and generators.\"\"\"\n",
    "\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "def read_mapper_output(file, separator='\\t'):\n",
    "    for line in file:\n",
    "        yield line.rstrip().split(separator, 1)\n",
    "\n",
    "def main(separator='\\t'):\n",
    "    # input comes from STDIN (standard input)\n",
    "    data = read_mapper_output(sys.stdin, separator=separator)\n",
    "    # groupby groups multiple word-count pairs by word,\n",
    "    # and creates an iterator that returns consecutive keys and their group:\n",
    "    #   current_word - string containing a word (the key)\n",
    "    #   group - iterator yielding all [\"&lt;current_word&gt;\", \"&lt;count&gt;\"] items\n",
    "    for current_word, group in groupby(data, itemgetter(0)):\n",
    "        try:\n",
    "            total_count = sum(int(count) for current_word, count in group)\n",
    "            print (\"%s%s%d\" % (current_word, separator, total_count))\n",
    "        except ValueError:\n",
    "            # count was not a number, so silently discard this item\n",
    "            pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
